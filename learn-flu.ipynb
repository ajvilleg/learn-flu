{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries: Inspect and Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajvilleg/miniforge3/envs/learn-flu/lib/python3.12/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries required\n",
    "\n",
    "# Data Processing and EDA\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For bioinformatics tasks\n",
    "from Bio import SeqIO\n",
    "# older alignment method\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# newer alignment method\n",
    "from Bio import Align\n",
    "from Bio.Align import PairwiseAligner\n",
    "import multiprocessing\n",
    "\n",
    "# For Machine Learning\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from seaborn import heatmap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "# Show all the output for every print not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# Configuration and settings\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# To check if in Google Colab\n",
    "from IPython.core.getipython import get_ipython\n",
    "# To display all the output in a nicer table\n",
    "from IPython.display import display\n",
    "# To time the execution of the code\n",
    "import time\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:02:52.715064\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ajvilleg/miniforge3/envs/learn-flu/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.2\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if 'google.colab' in str(get_ipython()):\n",
    "    # TODO: if running on Google Colab, install any packages you need to here. For example:\n",
    "    #!pip install unidecode\n",
    "    #!pip install category_encoders\n",
    "    #!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's minimize randomness\n",
    "# numpy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing the entire process\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the FASTA file\n",
    "records = list(SeqIO.parse(\"/home/ajvilleg/Netdrive/AI/GISAID/EpiFlu_Training/30-Jun-2024/gisaid_epiflu_sequence_2024-06-30.fasta\", \"fasta\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the details from the description of each record\n",
    "data = []\n",
    "\n",
    "# Iterate through records for every pair for NA and HA segments\n",
    "for record1, record2 in zip(records[::2], records[1::2]):\n",
    "    description1 = record1.description.split('|')\n",
    "    description2 = record2.description.split('|')\n",
    "\n",
    "    # Assume the isolate name is the same for both segments\n",
    "    isolate_name1 = description1[0].strip()\n",
    "    isolate_name2 = description2[0].strip()\n",
    "    if isolate_name1 != isolate_name2:\n",
    "        print(f\"Isolate names do not match: {isolate_name1} vs {isolate_name2}\")\n",
    "        raise ValueError(\"Isolate names do not match\")\n",
    "\n",
    "    # Assume the isolate ID is the same for both segments\n",
    "    isolate_id1 = description1[1].strip()\n",
    "    isolate_id2 = description2[1].strip()\n",
    "    if isolate_id1 != isolate_id2:\n",
    "        print(f\"Isolate IDs do not match: {isolate_id1} vs {isolate_id2}\")\n",
    "        raise ValueError(\"Isolate IDs do not match\")\n",
    "\n",
    "    # Assume the flu type is the same for both segments\n",
    "    flu_type1 = description1[2].strip()\n",
    "    flu_type2 = description2[2].strip()\n",
    "    if flu_type1 != flu_type2:\n",
    "        print(f\"Flu types do not match: {flu_type1} vs {flu_type2}\")\n",
    "        raise ValueError(\"Flu types do not match\")\n",
    "\n",
    "    # Assume the lineage is the same for both segments\n",
    "    lineage1 = description1[3].strip()\n",
    "    lineage2 = description2[3].strip()\n",
    "    if lineage1 != lineage2:\n",
    "        print(f\"Lineages do not match: {lineage1} vs {lineage2}\")\n",
    "        raise ValueError(\"Lineages do not match\")\n",
    "\n",
    "    # The segment labels are different for NA and HA segments\n",
    "    segment1 = description1[4].strip()\n",
    "    segment2 = description2[4].strip()  \n",
    "\n",
    "    # Assume the collection date is the same for both segments\n",
    "    collection_date1 = description1[5].strip()\n",
    "    collection_date2 = description2[5].strip()\n",
    "    if collection_date1 != collection_date2:\n",
    "        print(f\"Collection dates do not match: {collection_date1} vs {collection_date2}\")\n",
    "        raise ValueError(\"Collection dates do not match\")\n",
    "\n",
    "    # Assume the clade is the same for both segments. This is important as this will be our label for classification\n",
    "    clade1 = description1[6].strip()\n",
    "    clade2 = description2[6].strip()\n",
    "    if clade1 != clade2:\n",
    "        print(f\"Clades do not match: {clade1} vs {clade2}\")\n",
    "        raise ValueError(\"Clades do not match\")\n",
    "\n",
    "    # The sequences will be different corresopnding to the NA and HA segments\n",
    "    sequence1 = str(record1.seq)\n",
    "    sequence2 = str(record2.seq)\n",
    "    if segment1 == 'HA':\n",
    "        sequence_ha = sequence1\n",
    "        sequence_na = sequence2\n",
    "    else: # segment2 == 'HA'\n",
    "        sequence_ha = sequence2\n",
    "        sequence_na = sequence1\n",
    "    data.append([isolate_name1, isolate_id1, flu_type1, lineage1, sequence_ha, sequence_na, collection_date1, clade1])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Isolate_Name', 'Isolate_ID', 'Flu_Type', 'Lineage', 'HA', 'NA', 'Collection Date', 'Clade'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Isolate_Name</th>\n",
       "      <th>Isolate_ID</th>\n",
       "      <th>Flu_Type</th>\n",
       "      <th>Lineage</th>\n",
       "      <th>HA</th>\n",
       "      <th>NA</th>\n",
       "      <th>Collection Date</th>\n",
       "      <th>Clade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/Michigan/136/2018</td>\n",
       "      <td>EPI_ISL_360559</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>6B.1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A/USA/7C9/2010</td>\n",
       "      <td>EPI_ISL_17760636</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...</td>\n",
       "      <td>atgaatccaaaccaaaagataataaccattggttcgatctgtatga...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>6B.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/USA/7C7/2010</td>\n",
       "      <td>EPI_ISL_17760635</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...</td>\n",
       "      <td>atgaatccaaaccaaaagataataaccattggttcgatctgtatga...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>6B.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A/USA/7K3/1935</td>\n",
       "      <td>EPI_ISL_17760634</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...</td>\n",
       "      <td>atgaatccaaaccaaaagataataaccattggttcggtctgtatga...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>6B.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A/Michigan/98/2018</td>\n",
       "      <td>EPI_ISL_360571</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>6B.1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15414</th>\n",
       "      <td>A/Washington/97/2020</td>\n",
       "      <td>EPI_ISL_2588628</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15415</th>\n",
       "      <td>A/Washington/96/2020</td>\n",
       "      <td>EPI_ISL_2588627</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15416</th>\n",
       "      <td>A/Washington/90/2020</td>\n",
       "      <td>EPI_ISL_2588626</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggctctat...</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>6B.1A.5a.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15417</th>\n",
       "      <td>A/Pennsylvania/164/2020</td>\n",
       "      <td>EPI_ISL_2588633</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15418</th>\n",
       "      <td>A/Washington/91/2020</td>\n",
       "      <td>EPI_ISL_2588632</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15419 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Isolate_Name        Isolate_ID  Flu_Type Lineage  \\\n",
       "0          A/Michigan/136/2018    EPI_ISL_360559  A_/_H1N1   pdm09   \n",
       "1               A/USA/7C9/2010  EPI_ISL_17760636  A_/_H1N1   pdm09   \n",
       "2               A/USA/7C7/2010  EPI_ISL_17760635  A_/_H1N1   pdm09   \n",
       "3               A/USA/7K3/1935  EPI_ISL_17760634  A_/_H1N1   pdm09   \n",
       "4           A/Michigan/98/2018    EPI_ISL_360571  A_/_H1N1   pdm09   \n",
       "...                        ...               ...       ...     ...   \n",
       "15414     A/Washington/97/2020   EPI_ISL_2588628  A_/_H1N1   pdm09   \n",
       "15415     A/Washington/96/2020   EPI_ISL_2588627  A_/_H1N1   pdm09   \n",
       "15416     A/Washington/90/2020   EPI_ISL_2588626  A_/_H1N1   pdm09   \n",
       "15417  A/Pennsylvania/164/2020   EPI_ISL_2588633  A_/_H1N1   pdm09   \n",
       "15418     A/Washington/91/2020   EPI_ISL_2588632  A_/_H1N1   pdm09   \n",
       "\n",
       "                                                      HA  \\\n",
       "0      ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...   \n",
       "1      atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...   \n",
       "2      atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...   \n",
       "3      atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...   \n",
       "4      ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...   \n",
       "...                                                  ...   \n",
       "15414  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "15415  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "15416  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...   \n",
       "15417  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "15418  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "\n",
       "                                                      NA Collection Date  \\\n",
       "0      agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...      2018-01-16   \n",
       "1      atgaatccaaaccaaaagataataaccattggttcgatctgtatga...      2010-11-01   \n",
       "2      atgaatccaaaccaaaagataataaccattggttcgatctgtatga...      2010-11-01   \n",
       "3      atgaatccaaaccaaaagataataaccattggttcggtctgtatga...      2010-11-01   \n",
       "4      agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...      2018-01-08   \n",
       "...                                                  ...             ...   \n",
       "15414  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-04   \n",
       "15415  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-04   \n",
       "15416  agtttaaaatgaatccaaaccaaaagataataaccattggctctat...      2020-02-02   \n",
       "15417  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-24   \n",
       "15418  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-02   \n",
       "\n",
       "            Clade  \n",
       "0           6B.1A  \n",
       "1            6B.1  \n",
       "2            6B.1  \n",
       "3            6B.1  \n",
       "4           6B.1A  \n",
       "...           ...  \n",
       "15414  6B.1A.5a.2  \n",
       "15415  6B.1A.5a.2  \n",
       "15416  6B.1A.5a.1  \n",
       "15417  6B.1A.5a.2  \n",
       "15418  6B.1A.5a.2  \n",
       "\n",
       "[15419 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look at the data  \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15419 entries, 0 to 15418\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Isolate_Name     15419 non-null  object\n",
      " 1   Isolate_ID       15419 non-null  object\n",
      " 2   Flu_Type         15419 non-null  object\n",
      " 3   Lineage          15419 non-null  object\n",
      " 4   HA               15419 non-null  object\n",
      " 5   NA               15419 non-null  object\n",
      " 6   Collection Date  15419 non-null  object\n",
      " 7   Clade            15419 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 963.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15419 entries, 0 to 15418\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Isolate_Name     15419 non-null  object        \n",
      " 1   Isolate_ID       15419 non-null  object        \n",
      " 2   Flu_Type         15419 non-null  object        \n",
      " 3   Lineage          15419 non-null  object        \n",
      " 4   HA               15419 non-null  object        \n",
      " 5   NA               15419 non-null  object        \n",
      " 6   Collection Date  15419 non-null  datetime64[ns]\n",
      " 7   Clade            15419 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(7)\n",
      "memory usage: 963.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Convert all columns to strings except Collection Date\n",
    "df = df.astype(str)\n",
    "\n",
    "# Convert \"Collection Date\" column to date\n",
    "df[\"Collection Date\"] = pd.to_datetime(df[\"Collection Date\"])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collection Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2017-12-03 02:55:00.914456320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2009-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2016-03-03 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019-01-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2020-01-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-04-26 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Collection Date\n",
       "count                          15419\n",
       "mean   2017-12-03 02:55:00.914456320\n",
       "min              2009-01-01 00:00:00\n",
       "25%              2016-03-03 00:00:00\n",
       "50%              2019-01-23 00:00:00\n",
       "75%              2020-01-22 00:00:00\n",
       "max              2023-04-26 00:00:00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df has 1 duplicate rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Isolate_Name</th>\n",
       "      <th>Isolate_ID</th>\n",
       "      <th>Flu_Type</th>\n",
       "      <th>Lineage</th>\n",
       "      <th>HA</th>\n",
       "      <th>NA</th>\n",
       "      <th>Collection Date</th>\n",
       "      <th>Clade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14162</th>\n",
       "      <td>A/Houston/2OS/2009</td>\n",
       "      <td>EPI_ISL_63939</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>aaaagcaacaaaaatgaaggcaatactagtagttctgctatataca...</td>\n",
       "      <td>aaatgaatccaaaccaaaagataataaccattggttcggtctgtat...</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>6B.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Isolate_Name     Isolate_ID  Flu_Type Lineage  \\\n",
       "14162  A/Houston/2OS/2009  EPI_ISL_63939  A_/_H1N1   pdm09   \n",
       "\n",
       "                                                      HA  \\\n",
       "14162  aaaagcaacaaaaatgaaggcaatactagtagttctgctatataca...   \n",
       "\n",
       "                                                      NA Collection Date Clade  \n",
       "14162  aaatgaatccaaaccaaaagataataaccattggttcggtctgtat...      2009-05-18  6B.1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df has 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated rows in training data\n",
    "print(f'df has {df.duplicated().sum()} duplicate rows')\n",
    "display(df[df.duplicated()])\n",
    "# Drop duplicates and check again\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f'df has {df.duplicated().sum()} duplicate rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Missing values / NaN / Empty Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in df:\n",
      "Isolate_Name       0\n",
      "Isolate_ID         0\n",
      "Flu_Type           0\n",
      "Lineage            0\n",
      "HA                 0\n",
      "NA                 0\n",
      "Collection Date    0\n",
      "Clade              0\n",
      "dtype: int64\n",
      "\n",
      "Empty string values in df:\n",
      "Isolate_Name: 0\n",
      "Isolate_ID: 0\n",
      "Flu_Type: 0\n",
      "Lineage: 0\n",
      "HA: 0\n",
      "NA: 0\n",
      "Clade: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values and empty strings\n",
    "print(\"Missing values in df:\")\n",
    "print(df.isnull().sum())  # Check for NaN values\n",
    "print(\"\\nEmpty string values in df:\")\n",
    "for col in df.select_dtypes(include=['object']):  # Iterate over columns with string datatype\n",
    "    print(f\"{col}: {(df[col] == '').sum()}\")     # Count empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with empty strings in 'Clade' from the training data\n",
    "df.dropna(subset=['Clade'], inplace=True)  # Drop rows with NaN (including empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in df:\n",
      "Isolate_Name       0\n",
      "Isolate_ID         0\n",
      "Flu_Type           0\n",
      "Lineage            0\n",
      "HA                 0\n",
      "NA                 0\n",
      "Collection Date    0\n",
      "Clade              0\n",
      "dtype: int64\n",
      "\n",
      "Empty string values in df:\n",
      "Isolate_Name: 0\n",
      "Isolate_ID: 0\n",
      "Flu_Type: 0\n",
      "Lineage: 0\n",
      "HA: 0\n",
      "NA: 0\n",
      "Clade: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values and empty strings\n",
    "print(\"Missing values in df:\")\n",
    "print(df.isnull().sum())  # Check for NaN values\n",
    "print(\"\\nEmpty string values in df:\")\n",
    "for col in df.select_dtypes(include=['object']):  # Iterate over columns with string datatype\n",
    "    print(f\"{col}: {(df[col] == '').sum()}\")     # Count empty strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 Class imbalance in Clade column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clade\n",
       "6B.1             4862\n",
       "6B.1A.5a.2a.1    1914\n",
       "6B.1A.1          1576\n",
       "6B.1A.5a.1       1538\n",
       "6B.1A.5b         1304\n",
       "6B.1A.5a.2       1152\n",
       "6B.1A.5a          922\n",
       "6B.1A             590\n",
       "6B.1A.7           510\n",
       "6B.1A.5a.2a       373\n",
       "6B.1A.6           300\n",
       "6B.1A.3           206\n",
       "6B.1A.2            82\n",
       "6B.2               44\n",
       "6B.1A.5            39\n",
       "unassigned          6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Clade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with \"unassigned\" in 'Clade' from the training data\n",
    "df = df[df['Clade'] != 'unassigned']  # Filter out rows with label \"unassigned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clade\n",
       "6B.1             4862\n",
       "6B.1A.5a.2a.1    1914\n",
       "6B.1A.1          1576\n",
       "6B.1A.5a.1       1538\n",
       "6B.1A.5b         1304\n",
       "6B.1A.5a.2       1152\n",
       "6B.1A.5a          922\n",
       "6B.1A             590\n",
       "6B.1A.7           510\n",
       "6B.1A.5a.2a       373\n",
       "6B.1A.6           300\n",
       "6B.1A.3           206\n",
       "6B.1A.2            82\n",
       "6B.2               44\n",
       "6B.1A.5            39\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Clade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15412, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Look at sequence length stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths in HA columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    15412.000000\n",
       "mean      1741.636452\n",
       "std         22.450589\n",
       "min       1410.000000\n",
       "25%       1734.000000\n",
       "50%       1752.000000\n",
       "75%       1752.000000\n",
       "max       1922.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths in NA columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    15412.000000\n",
       "mean      1428.921555\n",
       "std         12.396946\n",
       "min       1410.000000\n",
       "25%       1420.000000\n",
       "50%       1433.000000\n",
       "75%       1433.000000\n",
       "max       1701.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sequence_length(row, column):\n",
    "    \"\"\"Calculates the length of the sequence in the specified column.\"\"\"\n",
    "    return len(row[column])\n",
    "\n",
    "ha_sequence_lengths = df.apply(get_sequence_length, axis=1, column=\"HA\")\n",
    "na_sequence_lengths = df.apply(get_sequence_length, axis=1, column=\"NA\")\n",
    "print(\"Sequence lengths in HA columns:\")\n",
    "ha_sequence_lengths.describe()\n",
    "print(\"Sequence lengths in NA columns:\")\n",
    "na_sequence_lengths.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 Extract HA1 Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start strings for HA1 and HA2 regions\n",
    "ha1_start = \"ATGAAGGCAATACTAGTAGTTATGCTGTATACATTTACAACCGCAAATGCA\"\n",
    "ha2_start = \"GGCCTATTCGGGGCCATTGCTGGCTTCATCGAAGGGGGGTGGACA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pairwise2.align.localmst - the older method\n",
    "def extract_ha1_with_alignment(ha_sequence):\n",
    "    # Align with HA1 start\n",
    "    ha1_alignments = pairwise2.align.localms(ha1_start.lower(), ha_sequence, 2, -1, -0.5, -0.1)\n",
    "    if ha1_alignments:\n",
    "        best_ha1_alignment = ha1_alignments[0]  # Get the best alignment\n",
    "        ha1_start_index = best_ha1_alignment[3] + len(ha1_start)  # Start after alignment\n",
    "\n",
    "        # Align with HA2 start\n",
    "        ha2_alignments = pairwise2.align.localms(ha2_start.lower(), ha_sequence, 2, -1, -0.5, -0.1)\n",
    "        if ha2_alignments:\n",
    "            best_ha2_alignment = ha2_alignments[0]\n",
    "            ha2_start_index = best_ha2_alignment[3]  # Exact match start\n",
    "            \n",
    "            ha1_sequence = ha_sequence[ha1_start_index:ha2_start_index]\n",
    "            return ha1_sequence\n",
    "        else:\n",
    "            return \"ERROR: HA2 region not found in alignment\"  # Or handle differently\n",
    "    else:\n",
    "        return \"ERROR: HA1 region not found in alignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using PairwiseAligner - the newer method, doesn't work\n",
    "# def extract_ha1_with_alignment(ha_sequence):\n",
    "#     aligner = PairwiseAligner()\n",
    "\n",
    "#     # Configure aligner for local alignment and scoring\n",
    "#     aligner.mode = 'local'\n",
    "#     aligner.match_score = 2\n",
    "#     aligner.mismatch_score = -1\n",
    "#     aligner.open_gap_score = -5   # Adjust gap penalties as needed\n",
    "#     aligner.extend_gap_score = -1\n",
    "\n",
    "#     # Align with HA1 start\n",
    "#     ha1_alignments = aligner.align(ha1_start.lower(), ha_sequence)\n",
    "\n",
    "#     if not ha1_alignments:\n",
    "#         # Try alternative extraction based on HA2 start\n",
    "#         ha2_start_index = ha_sequence.find(ha2_start)\n",
    "#         if ha2_start_index != -1:  # HA2 start found\n",
    "#             ha1_sequence = ha_sequence[:ha2_start_index]\n",
    "#             return ha1_sequence\n",
    "#         else:\n",
    "#             return \"ERROR: Neither HA1 nor HA2 start found\"  \n",
    "\n",
    "#     for alignment in ha1_alignments:  # Iterate over possible alignments\n",
    "#         ha1_start_index = alignment.aligned[0][1][0]  # Extract aligned region start\n",
    "#         ha1_end_index = alignment.aligned[0][1][1]    # Extract aligned region end\n",
    "        \n",
    "#         # Align with HA2 start (starting from the end of the HA1 alignment)\n",
    "#         ha2_alignments = aligner.align(ha2_start.lower(), ha_sequence[ha1_end_index:])\n",
    "#         for alignment in ha2_alignments:\n",
    "#             ha2_start_index = ha1_end_index + alignment.aligned[0][1][0]\n",
    "#             ha1_sequence = ha_sequence[ha1_start_index:ha2_start_index]\n",
    "#             return ha1_sequence\n",
    "        \n",
    "#     return \"ERROR: HA1 or HA2 region not found in alignment\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply alignment in parallel\n",
    "def parallel_extract_ha1(ha_sequences):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        return pool.map(extract_ha1_with_alignment, ha_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data file name that we will use \n",
    "# to save the extracted HA1 sequences so we can \n",
    "# just reload them later instead of rerunning the extraction which takes time\n",
    "data_file = \"train_flu_data_with_ha1_for_clade_prediction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data from train_flu_data_with_ha1_for_clade_prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if the data file exists\n",
    "if os.path.isfile(data_file):\n",
    "    print(f\"Loading pre-processed data from {data_file}\")\n",
    "    df = pd.read_csv(data_file)\n",
    "else:\n",
    "    print(\"Pre-processed data saved to external file not found. Performing sequence alignment and extraction...\")\n",
    "    # Single-threaded\n",
    "    # Apply to your DataFrame\n",
    "    # df['HA1'] = df['HA'].astype(str).apply(extract_ha1_with_alignment)\n",
    "\n",
    "    # Multi-threaded\n",
    "    # Apply the function in parallel (assuming your DataFrame is named 'df')\n",
    "    df['HA1'] = parallel_extract_ha1(df['HA'].astype(str).tolist())\n",
    "\n",
    "    # Show a sample alignment of the first HA sequence\n",
    "    print(\"\\n### Sample Alignment of the First HA Sequence:\\n\") \n",
    "    first_alignment = pairwise2.align.localms(ha1_start.lower(), df['HA'][0], 2, -1, -0.5, -0.1)[0]\n",
    "    print(format_alignment(*first_alignment))\n",
    "\n",
    "    # # Add HA1 Length Column\n",
    "    # df['HA1_length'] = df['HA1'].astype(str).apply(len)\n",
    "\n",
    "    # # Sort by HA1 Length\n",
    "    # df_sorted = df.sort_values(by='HA1_length')  # Sort in ascending order\n",
    "\n",
    "    # Save the processed data\n",
    "    print(\"Save the processed data to external file to speed up future runs.\")\n",
    "    df.to_csv(data_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Isolate_Name</th>\n",
       "      <th>Isolate_ID</th>\n",
       "      <th>Flu_Type</th>\n",
       "      <th>Lineage</th>\n",
       "      <th>HA</th>\n",
       "      <th>NA</th>\n",
       "      <th>Collection Date</th>\n",
       "      <th>Clade</th>\n",
       "      <th>HA1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/Michigan/136/2018</td>\n",
       "      <td>EPI_ISL_360559</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>6B.1A</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A/USA/7C9/2010</td>\n",
       "      <td>EPI_ISL_17760636</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...</td>\n",
       "      <td>atgaatccaaaccaaaagataataaccattggttcgatctgtatga...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>6B.1</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/USA/7C7/2010</td>\n",
       "      <td>EPI_ISL_17760635</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...</td>\n",
       "      <td>atgaatccaaaccaaaagataataaccattggttcgatctgtatga...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>6B.1</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A/USA/7K3/1935</td>\n",
       "      <td>EPI_ISL_17760634</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...</td>\n",
       "      <td>atgaatccaaaccaaaagataataaccattggttcggtctgtatga...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>6B.1</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A/Michigan/98/2018</td>\n",
       "      <td>EPI_ISL_360571</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>6B.1A</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15407</th>\n",
       "      <td>A/Washington/97/2020</td>\n",
       "      <td>EPI_ISL_2588628</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15408</th>\n",
       "      <td>A/Washington/96/2020</td>\n",
       "      <td>EPI_ISL_2588627</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15409</th>\n",
       "      <td>A/Washington/90/2020</td>\n",
       "      <td>EPI_ISL_2588626</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggctctat...</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>6B.1A.5a.1</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15410</th>\n",
       "      <td>A/Pennsylvania/164/2020</td>\n",
       "      <td>EPI_ISL_2588633</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15411</th>\n",
       "      <td>A/Washington/91/2020</td>\n",
       "      <td>EPI_ISL_2588632</td>\n",
       "      <td>A_/_H1N1</td>\n",
       "      <td>pdm09</td>\n",
       "      <td>ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...</td>\n",
       "      <td>agtttaaaatgaatccaaaccaaaagataataaccattggttctat...</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>6B.1A.5a.2</td>\n",
       "      <td>gacacattatgtataggttatcatgcgaacaattcaacagacactg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15412 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Isolate_Name        Isolate_ID  Flu_Type Lineage  \\\n",
       "0          A/Michigan/136/2018    EPI_ISL_360559  A_/_H1N1   pdm09   \n",
       "1               A/USA/7C9/2010  EPI_ISL_17760636  A_/_H1N1   pdm09   \n",
       "2               A/USA/7C7/2010  EPI_ISL_17760635  A_/_H1N1   pdm09   \n",
       "3               A/USA/7K3/1935  EPI_ISL_17760634  A_/_H1N1   pdm09   \n",
       "4           A/Michigan/98/2018    EPI_ISL_360571  A_/_H1N1   pdm09   \n",
       "...                        ...               ...       ...     ...   \n",
       "15407     A/Washington/97/2020   EPI_ISL_2588628  A_/_H1N1   pdm09   \n",
       "15408     A/Washington/96/2020   EPI_ISL_2588627  A_/_H1N1   pdm09   \n",
       "15409     A/Washington/90/2020   EPI_ISL_2588626  A_/_H1N1   pdm09   \n",
       "15410  A/Pennsylvania/164/2020   EPI_ISL_2588633  A_/_H1N1   pdm09   \n",
       "15411     A/Washington/91/2020   EPI_ISL_2588632  A_/_H1N1   pdm09   \n",
       "\n",
       "                                                      HA  \\\n",
       "0      ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...   \n",
       "1      atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...   \n",
       "2      atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...   \n",
       "3      atgaaggcaatactagtagttctgctatatacatttgcaaccgcaa...   \n",
       "4      ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...   \n",
       "...                                                  ...   \n",
       "15407  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "15408  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "15409  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttctgct...   \n",
       "15410  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "15411  ggaaaacaaaagcaacaaaaatgaaggcaatactagtagttatgct...   \n",
       "\n",
       "                                                      NA Collection Date  \\\n",
       "0      agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...      2018-01-16   \n",
       "1      atgaatccaaaccaaaagataataaccattggttcgatctgtatga...      2010-11-01   \n",
       "2      atgaatccaaaccaaaagataataaccattggttcgatctgtatga...      2010-11-01   \n",
       "3      atgaatccaaaccaaaagataataaccattggttcggtctgtatga...      2010-11-01   \n",
       "4      agtttaaaatgaatccaaaccaaaagataataaccattggttcgat...      2018-01-08   \n",
       "...                                                  ...             ...   \n",
       "15407  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-04   \n",
       "15408  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-04   \n",
       "15409  agtttaaaatgaatccaaaccaaaagataataaccattggctctat...      2020-02-02   \n",
       "15410  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-24   \n",
       "15411  agtttaaaatgaatccaaaccaaaagataataaccattggttctat...      2020-02-02   \n",
       "\n",
       "            Clade                                                HA1  \n",
       "0           6B.1A  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "1            6B.1  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "2            6B.1  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "3            6B.1  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "4           6B.1A  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "...           ...                                                ...  \n",
       "15407  6B.1A.5a.2  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "15408  6B.1A.5a.2  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "15409  6B.1A.5a.1  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "15410  6B.1A.5a.2  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "15411  6B.1A.5a.2  gacacattatgtataggttatcatgcgaacaattcaacagacactg...  \n",
       "\n",
       "[15412 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the modified DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15412 entries, 0 to 15411\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Isolate_Name     15412 non-null  object\n",
      " 1   Isolate_ID       15412 non-null  object\n",
      " 2   Flu_Type         15412 non-null  object\n",
      " 3   Lineage          15412 non-null  object\n",
      " 4   HA               15412 non-null  object\n",
      " 5   NA               15412 non-null  object\n",
      " 6   Collection Date  15412 non-null  object\n",
      " 7   Clade            15412 non-null  object\n",
      " 8   HA1              15410 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop any empty HA1 subregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15412, 9)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with empty strings in 'HA1' from the training data\n",
    "# This step only works when the extraction is run, not when loading the saved extraction file\n",
    "df = df[(df['HA1'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15412, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in 'HA1' from the training data\n",
    "# This step only works when loading the saved extraction file, not when running the extraction\n",
    "# Excel adds NaN values when saving empty strings\n",
    "df.dropna(subset=['HA1'], inplace=True)  # Drop rows with NaN (including empty strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15410, 9)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths in HA1 columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    15410.000000\n",
       "mean       980.796820\n",
       "std          3.101824\n",
       "min        900.000000\n",
       "25%        981.000000\n",
       "50%        981.000000\n",
       "75%        981.000000\n",
       "max        981.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha1_sequence_lengths = df.apply(get_sequence_length, axis=1, column=\"HA1\")\n",
    "print(\"Sequence lengths in HA1 columns:\")\n",
    "ha1_sequence_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows where HA1 subregion < 981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ha1_sequence_lengths >= 981]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15323, 9)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths in HA1 columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    15323.0\n",
       "mean       981.0\n",
       "std          0.0\n",
       "min        981.0\n",
       "25%        981.0\n",
       "50%        981.0\n",
       "75%        981.0\n",
       "max        981.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha1_sequence_lengths = df.apply(get_sequence_length, axis=1, column=\"HA1\")\n",
    "print(\"Sequence lengths in HA1 columns:\")\n",
    "ha1_sequence_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-mers and k-mer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define k-mer length\n",
    "kmer_length = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract kmers (can be reused)\n",
    "def get_kmers(sequence, k):\n",
    "  \"\"\"\n",
    "  Extracts all k-mers (subsequences of length k) from a DNA sequence.\n",
    "  \"\"\"\n",
    "  kmers = []\n",
    "  for i in range(len(sequence) - k + 1):\n",
    "    kmer = sequence[i:i+k]\n",
    "    kmers.append(kmer)\n",
    "  return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store kmers for each sequence (identified by row index)\n",
    "kmer_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract k-mers with length kmer_length from each sequence and store them in the dictionary\n",
    "for i, row in df.iterrows():\n",
    "  # Extract kmers from HA sequence (assuming it exists)\n",
    "  ha1_kmers = []\n",
    "  if \"HA1\" in row:  # Check if \"HA1\" column exists\n",
    "    sequence = str(row[\"HA1\"])\n",
    "    ha_kmers = get_kmers(sequence, kmer_length)\n",
    "\n",
    "  # # Extract kmers from NA sequence (assuming it exists)\n",
    "  # na_kmers = []\n",
    "  # if \"NA\" in row:  # Check if \"NA\" column exists\n",
    "  #   sequence = str(row[\"NA\"])\n",
    "  #   na_kmers = get_kmers(sequence, kmer_length)\n",
    "\n",
    "  # Store kmers separately in the dictionary\n",
    "  kmer_dict[i] = {\n",
    "      \"HA\": ha_kmers,\n",
    "      # \"NA\": na_kmers,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Count-based Encoding using chunking to optimize memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk size (adjust as needed)\n",
    "chunk_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_dict):\n",
    "    \"\"\"\n",
    "    Processes a chunk of data from the kmer_dict and returns count-based features,\n",
    "    keeping HA and NA k-mer counts separate to avoid overlap issues.\n",
    "    \"\"\"\n",
    "    chunk_ha_features, chunk_na_features = [], []\n",
    "\n",
    "    for kmer_dict_row in chunk_dict.values():\n",
    "        # Extract HA and NA kmers\n",
    "        ha_kmers = kmer_dict_row[\"HA\"]\n",
    "        na_kmers = kmer_dict_row[\"NA\"]\n",
    "\n",
    "        # Count occurrences of kmers for HA and NA separately\n",
    "        ha_kmer_counts = Counter(ha_kmers)\n",
    "        na_kmer_counts = Counter(na_kmers)\n",
    "\n",
    "        # Extract counts and append as features (ensure consistent order for all chunks)\n",
    "        all_kmers = list(set(ha_kmer_counts.keys()) | set(na_kmer_counts.keys()))  # Get unique k-mers\n",
    "\n",
    "        chunk_ha_features.append([ha_kmer_counts[kmer] if kmer in ha_kmer_counts else 0 for kmer in all_kmers])\n",
    "        chunk_na_features.append([na_kmer_counts[kmer] if kmer in na_kmer_counts else 0 for kmer in all_kmers])\n",
    "        # END New code tweak\n",
    "\n",
    "    return chunk_ha_features, chunk_na_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through kmer_dict in chunks\n",
    "ha_features = []\n",
    "na_features = []\n",
    "for i in range(0, len(kmer_dict), chunk_size):\n",
    "  # Get a chunk of data\n",
    "  chunk_dict = dict(list(kmer_dict.items())[i:i + chunk_size])\n",
    "\n",
    "  # Process features for the chunk\n",
    "  chunk_ha_features, chunk_na_features = process_chunk(chunk_dict)\n",
    "\n",
    "  # Append features from the chunk\n",
    "  ha_features.extend(chunk_ha_features)\n",
    "  na_features.extend(chunk_na_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define X and y and Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "# Assuming ha_features and na_features contain lists\n",
    "ha_features = [np.array(l) for l in ha_features]  # Convert lists to arrays\n",
    "na_features = [np.array(l) for l in na_features]  # Convert lists to arrays\n",
    "\n",
    "# The arrays in ha_features and na_features have different shapes, we need to ensure that the arrays in ha_features and na_features have the same shape before concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad shorter arrays in ha_features and na_features with zeros\n",
    "# Find the maximum number of features across all arrays\n",
    "max_feature_count = max(arr.shape[0] for arr in ha_features + na_features)\n",
    "\n",
    "# Pad shorter arrays in ha_features and na_features with zeros\n",
    "for i in range(len(ha_features)):\n",
    "  ha_features[i] = np.pad(ha_features[i], (0, max_feature_count - len(ha_features[i])), mode='constant')\n",
    "\n",
    "for i in range(len(na_features)):\n",
    "  na_features[i] = np.pad(na_features[i], (0, max_feature_count - len(na_features[i])), mode='constant')\n",
    "\n",
    "# Now concatenate by sample/instance (axis=1, instead of axis=0 which is stacking on top of each other) the padded arrays (assuming na_features have consistent shapes)\n",
    "X = np.concatenate((ha_features, na_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder to all unique classes (call only once)\n",
    "le.fit(df['Clade'])\n",
    "\n",
    "for index in df.index:\n",
    "  clade_label = le.transform(np.array([df.loc[index, \"Clade\"]]))[0]\n",
    "  y.append(clade_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Set random_state for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a Logistic Regression model\n",
    "# model = LogisticRegression(multi_class='ovr', solver='lbfgs')\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust hyperparameters\n",
    "model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a Random Forest model with balanced class weights to handle class imbalance\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced') \n",
    "# model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: Proportion of correctly predicted samples\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision: Ratio of true positives to all predicted positives\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Weighted average for multi-class\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Recall: Ratio of true positives to all actual positives\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Weighted average for multi-class\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# F1-score: Harmonic mean of precision and recall\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Weighted average for multi-class\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix with Seaborn\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a new figure for the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create heatmap using seaborn\n",
    "heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")  # Customize heatmap with annotations, format, and colormap\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predicted Clade\")\n",
    "plt.ylabel(\"True Clade\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show the confusion matrix\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End the timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End timing\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print total training and validation runtime\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTotal training and validation runtime: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load and Prepare Unseen Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the FASTA file for the new dataset\n",
    "test_records = list(SeqIO.parse(\"/home/ajvilleg/Netdrive/AI/GISAID/EpiFlu_Test/04-Jul-2024_Canada/gisaid_epiflu_sequence_04-Jul-2024_Canada.fasta\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate through records for every pair for NA and HA segments\n",
    "for record1, record2 in zip(test_records[::2], test_records[1::2]):\n",
    "    description1 = record1.description.split('|')\n",
    "    description2 = record2.description.split('|')\n",
    "\n",
    "    # Assume the isolate name is the same for both segments\n",
    "    isolate_name1 = description1[0].strip()\n",
    "    isolate_name2 = description2[0].strip()\n",
    "    if isolate_name1 != isolate_name2:\n",
    "        print(f\"Isolate names do not match: {isolate_name1} vs {isolate_name2}\")\n",
    "        raise ValueError(\"Isolate names do not match\")\n",
    "\n",
    "    # Assume the isolate ID is the same for both segments\n",
    "    isolate_id1 = description1[1].strip()\n",
    "    isolate_id2 = description2[1].strip()\n",
    "    if isolate_id1 != isolate_id2:\n",
    "        print(f\"Isolate IDs do not match: {isolate_id1} vs {isolate_id2}\")\n",
    "        raise ValueError(\"Isolate IDs do not match\")\n",
    "\n",
    "    # Assume the flu type is the same for both segments\n",
    "    flu_type1 = description1[2].strip()\n",
    "    flu_type2 = description2[2].strip()\n",
    "    if flu_type1 != flu_type2:\n",
    "        print(f\"Flu types do not match: {flu_type1} vs {flu_type2}\")\n",
    "        raise ValueError(\"Flu types do not match\")\n",
    "\n",
    "    # Assume the lineage is the same for both segments\n",
    "    lineage1 = description1[3].strip()\n",
    "    lineage2 = description2[3].strip()\n",
    "    if lineage1 != lineage2:\n",
    "        print(f\"Lineages do not match: {lineage1} vs {lineage2}\")\n",
    "        raise ValueError(\"Lineages do not match\")\n",
    "\n",
    "    # The segment labels are different for NA and HA segments\n",
    "    segment1 = description1[4].strip()\n",
    "    segment2 = description2[4].strip()  \n",
    "\n",
    "    # Assume the collection date is the same for both segments\n",
    "    collection_date1 = description1[5].strip()\n",
    "    collection_date2 = description2[5].strip()\n",
    "    if collection_date1 != collection_date2:\n",
    "        print(f\"Collection dates do not match: {collection_date1} vs {collection_date2}\")\n",
    "        raise ValueError(\"Collection dates do not match\")\n",
    "\n",
    "    # Assume the clade is the same for both segments. This is important as this will be our label for classification\n",
    "    clade1 = description1[6].strip()\n",
    "    clade2 = description2[6].strip()\n",
    "    if clade1 != clade2:\n",
    "        print(f\"Clades do not match: {clade1} vs {clade2}\")\n",
    "        raise ValueError(\"Clades do not match\")\n",
    "\n",
    "    # The sequences will be different corresopnding to the NA and HA segments\n",
    "    sequence1 = str(record1.seq)\n",
    "    sequence2 = str(record2.seq)\n",
    "    if segment1 == 'HA':\n",
    "        sequence_ha = sequence1\n",
    "        sequence_na = sequence2\n",
    "    else: # segment2 == 'HA'\n",
    "        sequence_ha = sequence2\n",
    "        sequence_na = sequence1\n",
    "    new_data.append([isolate_name1, isolate_id1, flu_type1, lineage1, sequence_ha, sequence_na, collection_date1, clade1])\n",
    "\n",
    "new_df = pd.DataFrame(new_data, columns=['Isolate_Name', 'Isolate_ID', 'Flu_Type', 'Lineage', 'HA', 'NA', 'Collection Date', 'Clade'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to strings except Collection Date\n",
    "new_df = new_df.astype(str)\n",
    "\n",
    "# Convert \"Collection Date\" column to date\n",
    "new_df[\"Collection Date\"] = pd.to_datetime(new_df[\"Collection Date\"])\n",
    "\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows in training data\n",
    "print(f'new_df has {new_df.duplicated().sum()} duplicate rows')\n",
    "display(new_df[new_df.duplicated()])\n",
    "# Drop duplicates and check again\n",
    "new_df.drop_duplicates(inplace=True)\n",
    "print(f'new_df has {new_df.duplicated().sum()} duplicate rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and empty strings\n",
    "print(\"Missing values in new_df:\")\n",
    "print(new_df.isnull().sum())  # Check for NaN values\n",
    "print(\"\\nEmpty string values in new_df:\")\n",
    "for col in new_df.select_dtypes(include=['object']):  # Iterate over columns with string datatype\n",
    "    print(f\"{col}: {(new_df[col] == '').sum()}\")     # Count empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ha_sequence_lengths = new_df.apply(get_sequence_length, axis=1, column=\"HA\")\n",
    "new_na_sequence_lengths = new_df.apply(get_sequence_length, axis=1, column=\"NA\")\n",
    "print(\"Sequence lengths in HA columns:\")\n",
    "new_ha_sequence_lengths.describe()\n",
    "print(\"Sequence lengths in NA columns:\")\n",
    "new_na_sequence_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Clade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store kmers for each sequence in the new dataset (identified by row index)\n",
    "new_kmer_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract k-mers from each sequence and store them in the dictionary\n",
    "for i, row in new_df.iterrows():\n",
    "    # Extract kmers from HA sequence (assuming it exists)\n",
    "    ha_kmers = []\n",
    "    if \"HA\" in row:  # Check if \"HA\" column exists\n",
    "        sequence = str(row[\"HA\"])\n",
    "        ha_kmers = get_kmers(sequence, kmer_length)\n",
    "\n",
    "    # Extract kmers from NA sequence (assuming it exists)\n",
    "    na_kmers = []\n",
    "    if \"NA\" in row:  # Check if \"NA\" column exists\n",
    "        sequence = str(row[\"NA\"])\n",
    "        na_kmers = get_kmers(sequence, kmer_length)\n",
    "\n",
    "    # Store kmers separately in the dictionary\n",
    "    new_kmer_dict[i] = {\n",
    "        \"HA\": ha_kmers,\n",
    "        \"NA\": na_kmers,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count-based Encoding on new data using same kmer chunking logic as before to optimize memory usage\n",
    "new_ha_features = []\n",
    "new_na_features = []\n",
    "for i in range(0, len(new_kmer_dict), chunk_size):\n",
    "    # Get a chunk of data\n",
    "    chunk_dict = dict(list(new_kmer_dict.items())[i:i + chunk_size])\n",
    "\n",
    "    # Process features for the chunk\n",
    "    chunk_ha_features, chunk_na_features = process_chunk(chunk_dict)\n",
    "\n",
    "    # Append features from the chunk\n",
    "    new_ha_features.extend(chunk_ha_features)\n",
    "    new_na_features.extend(chunk_na_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = []\n",
    "\n",
    "# Assuming ha_features and na_features contain lists\n",
    "new_ha_features = [np.array(l) for l in new_ha_features]  # Convert lists to arrays\n",
    "new_na_features = [np.array(l) for l in new_na_features]  # Convert lists to arrays\n",
    "\n",
    "# The arrays in ha_features and na_features have different shapes, we need to ensure that the arrays in new_ha_features and new_na_features have the same shape before concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad shorter arrays in new_ha_features and new_na_features with zeros\n",
    "# Find the maximum number of features across all arrays\n",
    "# new_max_feature_count = max(arr.shape[0] for arr in new_ha_features + new_na_features)\n",
    "\n",
    "# Pad shorter arrays in new_ha_features and new_na_features with zeros\n",
    "for i in range(len(new_ha_features)):\n",
    "    new_ha_features[i] = np.pad(new_ha_features[i], (0, max_feature_count - len(new_ha_features[i])), mode='constant')\n",
    "\n",
    "for i in range(len(new_na_features)):\n",
    "    new_na_features[i] = np.pad(new_na_features[i], (0, max_feature_count - len(new_na_features[i])), mode='constant')\n",
    "    \n",
    "X_new = np.concatenate((new_ha_features, new_na_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for the new data (new_df)\n",
    "le_new = LabelEncoder()\n",
    "\n",
    "# Re-fit the encoder with unique values from the new data ONLY\n",
    "le_new.fit(new_df['Clade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "y_pred_new = model.predict(X_new)\n",
    "\n",
    "# Decode predictions to get the original Clade labels\n",
    "predicted_clades = le_new.inverse_transform(y_pred_new)\n",
    "\n",
    "# Add predicted clades back to new_df\n",
    "new_df['Predicted_Clade'] = predicted_clades\n",
    "\n",
    "# Display the data with predictions and true clades\n",
    "display(new_df[['Isolate_Name', 'Isolate_ID', 'Collection Date', 'Clade', 'Predicted_Clade']])  # Display true and predicted clades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and display metrics \n",
    "y_true_new = le_new.transform(new_df['Clade']) # Encode true labels\n",
    "\n",
    "# Evaluate Predictions\n",
    "print(\"\\n### Model Evaluation on Test Dataset ###\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true_new, y_pred_new)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true_new, y_pred_new, average='weighted') \n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true_new, y_pred_new, average='weighted')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# F1-score\n",
    "f1 = f1_score(y_true_new, y_pred_new, average='weighted')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_new, y_pred_new))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true_new, y_pred_new)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Clade\")\n",
    "plt.ylabel(\"True Clade\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-flu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
