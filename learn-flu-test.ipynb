{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries: Inspect and Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries required\n",
    "\n",
    "# Data Processing and EDA\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For bioinformatics tasks\n",
    "from Bio import SeqIO\n",
    "# older alignment method\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# newer alignment method - not using this, \n",
    "# but just don't want to forget this option\n",
    "from Bio import Align\n",
    "from Bio.Align import PairwiseAligner\n",
    "import multiprocessing\n",
    "\n",
    "# For Machine Learning\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from seaborn import heatmap\n",
    "from sklearn.inspection import partial_dependence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "# Show all the output for every print not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# Configuration and settings\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# To check if in Google Colab\n",
    "from IPython.core.getipython import get_ipython\n",
    "# To display all the output in a nicer table\n",
    "from IPython.display import display\n",
    "# To time the execution of the code\n",
    "import time\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if 'google.colab' in str(get_ipython()):\n",
    "    # TODO: if running on Google Colab, install any packages you need to here. For example:\n",
    "    #!pip install unidecode\n",
    "    #!pip install category_encoders\n",
    "    #!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's minimize randomness\n",
    "# numpy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # For saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_headers(records):\n",
    "    \"\"\"Checks for duplicate FASTA headers in a list of SeqIO records.\n",
    "\n",
    "    Args:\n",
    "        records: A list of SeqRecord objects.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of indices corresponding to duplicate records.\n",
    "        list: A list of duplicate headers.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    duplicate_headers = []\n",
    "    for i, record in enumerate(records):\n",
    "        header = record.description  # Or use record.id\n",
    "        if header in seen:\n",
    "            duplicates.append(i)\n",
    "            duplicate_headers.append(header)\n",
    "        else:\n",
    "            seen.add(header)\n",
    "    return duplicates, duplicate_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_length(row, column):\n",
    "    \"\"\"Calculates the length of the sequence in the specified column.\"\"\"\n",
    "    return len(row[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define k-mer length\n",
    "# kmer_length = 12\n",
    "kmer_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract kmers (can be reused)\n",
    "def get_kmers(sequence, k):\n",
    "  \"\"\"\n",
    "  Extracts all k-mers (subsequences of length k) from a DNA sequence.\n",
    "  \"\"\"\n",
    "  kmers = []\n",
    "  for i in range(len(sequence) - k + 1):\n",
    "    kmer = sequence[i:i+k]\n",
    "    kmers.append(kmer)\n",
    "  return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk size (adjust as needed)\n",
    "chunk_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_dict):\n",
    "    \"\"\"\n",
    "    Processes a chunk of data from the kmer_dict and returns one-hot encoded features.\n",
    "    \"\"\"\n",
    "    chunk_ha_features = []\n",
    "\n",
    "    # Get unique k-mers across all sequences in the chunk\n",
    "    # Only run this code if 'ohe' attribute does not exist. This is to ensure that the one-hot encoder is only fit once.\n",
    "    # So, for the test data, 'ohe' already defined, this code will not run (will not do a fit on a new OHE) and go directly to the transformation step below\n",
    "    if not hasattr(process_chunk, 'ohe'):\n",
    "        all_kmers = set()\n",
    "        for kmer_dict_row in chunk_dict.values():\n",
    "            ha_kmers = kmer_dict_row[\"HA\"]\n",
    "            all_kmers.update(ha_kmers)\n",
    "\n",
    "        # Create one-hot encoder (only fit on the first chunk for consistent categories)\n",
    "        process_chunk.ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        process_chunk.ohe.fit([[kmer] for kmer in list(all_kmers)])  # Fit on unique kmers\n",
    "\n",
    "    # Transform each sequence into a one-hot encoded vector\n",
    "    for kmer_dict_row in chunk_dict.values():\n",
    "        ha_kmers = kmer_dict_row[\"HA\"]\n",
    "        kmer_indices = process_chunk.ohe.transform([[kmer] for kmer in ha_kmers]).sum(axis=0)\n",
    "        chunk_ha_features.append(kmer_indices)\n",
    "\n",
    "    return chunk_ha_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_runtime(total_time):\n",
    "    \"\"\"\n",
    "    Formats a given runtime (in seconds) into a human-readable string, \n",
    "    omitting zero-value components (days, hours, minutes).\n",
    "\n",
    "    Args:\n",
    "        total_time (float): The total runtime in seconds.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string representing the runtime.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert total time in seconds to a timedelta object\n",
    "    td = datetime.timedelta(seconds=total_time)\\\n",
    "    \n",
    "    # Extract days, hours, minutes, and seconds from the timedelta\n",
    "    days = td.days\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    # Create a list to store non-zero time components and their labels\n",
    "    time_components = []\n",
    "    if days > 0:\n",
    "        time_components.append(f\"{days} days\")\n",
    "    if hours > 0:\n",
    "        time_components.append(f\"{hours} hours\")\n",
    "    if minutes > 0:\n",
    "        time_components.append(f\"{minutes} minutes\")\n",
    "    time_components.append(f\"{seconds:.2f} seconds\")  # Always include seconds\n",
    "\n",
    "    # Join the time components with commas and \"and\"\n",
    "    formatted_time = \", \".join(time_components[:-1])  # Join all but the last\n",
    "    if len(time_components) > 1:\n",
    "        formatted_time += \" and \" + time_components[-1]  # Add \"and\" and the last\n",
    "    else:\n",
    "        formatted_time = time_components[0]  # If only one component, use it directly\n",
    "\n",
    "    return formatted_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and LabelEncoder\n",
    "model_filename = \"model_learn-flu-train_2024-08-19_01:50:52.joblib\" # Update the filename of the trained model as required\n",
    "model_data = joblib.load(model_filename)\n",
    "model = model_data['model']\n",
    "le = model_data['label_encoder']\n",
    "process_chunk.ohe = model_data['one_hot_encoder'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load and Prepare Unseen Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the FASTA file for the new dataset\n",
    "test_records = list(SeqIO.parse(\"/home/ajvilleg/Netdrive/AI/GISAID/EpiFlu_Test/11-Aug-2024_Oceania/gisaid_epiflu_sequence_11-Aug-2024.fasta\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates before extracting details\n",
    "duplicate_indices, duplicate_headers = check_duplicate_headers(test_records)\n",
    "if duplicate_indices:\n",
    "    print(\"Warning: Found the following duplicate FASTA headers:\")\n",
    "    for header in duplicate_headers:\n",
    "        print(header)\n",
    "    print(\"One copy of each duplicate record will be kept.\")\n",
    "\n",
    "    # Create a set of unique indices to keep\n",
    "    indices_to_keep = set(range(len(test_records))) - set(duplicate_indices)\n",
    "\n",
    "    # Filter the records list\n",
    "    test_records = [record for i, record in enumerate(test_records) if i in indices_to_keep]\n",
    "    print(\"Duplicate records have been removed, keeping one copy of each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the details from the description of each record\n",
    "test_data = []\n",
    "\n",
    "# Iterate through records for every pair for NA and HA segments\n",
    "for record1, record2 in zip(test_records[::2], test_records[1::2]):\n",
    "    description1 = record1.description.split('|')\n",
    "    description2 = record2.description.split('|')\n",
    "\n",
    "    # Assume the isolate name is the same for both segments\n",
    "    isolate_name1 = description1[0].strip()\n",
    "    isolate_name2 = description2[0].strip()\n",
    "    if isolate_name1 != isolate_name2:\n",
    "        print(f\"Isolate names do not match: {isolate_name1} vs {isolate_name2}\")\n",
    "        raise ValueError(\"Isolate names do not match\")\n",
    "\n",
    "    # Assume the isolate ID is the same for both segments\n",
    "    isolate_id1 = description1[1].strip()\n",
    "    isolate_id2 = description2[1].strip()\n",
    "    if isolate_id1 != isolate_id2:\n",
    "        print(f\"Isolate IDs do not match: {isolate_id1} vs {isolate_id2}\")\n",
    "        raise ValueError(\"Isolate IDs do not match\")\n",
    "\n",
    "    # Assume the flu type is the same for both segments\n",
    "    flu_type1 = description1[2].strip()\n",
    "    flu_type2 = description2[2].strip()\n",
    "    if flu_type1 != flu_type2:\n",
    "        print(f\"Flu types do not match: {flu_type1} vs {flu_type2}\")\n",
    "        raise ValueError(\"Flu types do not match\")\n",
    "\n",
    "    # Assume the lineage is the same for both segments\n",
    "    lineage1 = description1[3].strip()\n",
    "    lineage2 = description2[3].strip()\n",
    "    if lineage1 != lineage2:\n",
    "        print(f\"Lineages do not match: {lineage1} vs {lineage2}\")\n",
    "        raise ValueError(\"Lineages do not match\")\n",
    "\n",
    "    # The segment labels are different for NA and HA segments\n",
    "    segment1 = description1[4].strip()\n",
    "    segment2 = description2[4].strip()  \n",
    "\n",
    "    # Assume the collection date is the same for both segments\n",
    "    collection_date1 = description1[5].strip()\n",
    "    collection_date2 = description2[5].strip()\n",
    "    if collection_date1 != collection_date2:\n",
    "        print(f\"Collection dates do not match: {collection_date1} vs {collection_date2}\")\n",
    "        raise ValueError(\"Collection dates do not match\")\n",
    "\n",
    "    # Assume the clade is the same for both segments. This is important as this will be our label for classification\n",
    "    clade1 = description1[6].strip()\n",
    "    clade2 = description2[6].strip()\n",
    "    if clade1 != clade2:\n",
    "        print(f\"Clades do not match: {clade1} vs {clade2}\")\n",
    "        raise ValueError(\"Clades do not match\")\n",
    "\n",
    "    # The sequences will be different corresopnding to the NA and HA segments\n",
    "    sequence1 = str(record1.seq)\n",
    "    sequence2 = str(record2.seq)\n",
    "    if segment1 == 'HA':\n",
    "        sequence_ha = sequence1\n",
    "        sequence_na = sequence2\n",
    "    else: # segment2 == 'HA'\n",
    "        sequence_ha = sequence2\n",
    "        sequence_na = sequence1\n",
    "    test_data.append([isolate_name1, isolate_id1, flu_type1, lineage1, sequence_ha, sequence_na, collection_date1, clade1])\n",
    "\n",
    "test_df = pd.DataFrame(test_data, columns=['Isolate_Name', 'Isolate_ID', 'Flu_Type', 'Lineage', 'HA', 'NA', 'Collection Date', 'Clade'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Type using regular expressions\n",
    "test_df['Type'] = test_df['Flu_Type'].astype(str).str.extract(r'(A|B|C)').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract H_Subtype and N_Subtype with updated regex, allowing for one or more digits after H or N.\n",
    "test_df['H_Subtype'] = test_df['Flu_Type'].astype(str).str.extract(r'(H\\d+)').fillna('')\n",
    "test_df['N_Subtype'] = test_df['Flu_Type'].astype(str).str.extract(r'(N\\d+)').fillna('')\n",
    "print(test_df['H_Subtype'].value_counts().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "print(test_df['N_Subtype'].value_counts().to_markdown(numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data again\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to strings except Collection Date\n",
    "test_df = test_df.astype(str)\n",
    "\n",
    "# Convert \"Collection Date\" column to date\n",
    "test_df[\"Collection Date\"] = pd.to_datetime(test_df[\"Collection Date\"])\n",
    "\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows in test data\n",
    "print(f'test_df has {test_df.duplicated().sum()} duplicate rows')\n",
    "display(test_df[test_df.duplicated()])\n",
    "# Drop duplicates and check again\n",
    "test_df.drop_duplicates(inplace=True)\n",
    "print(f'test_df has {test_df.duplicated().sum()} duplicate rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Missing values / NaN / Empty Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and empty strings\n",
    "print(\"NaN values in test_df:\")\n",
    "print(test_df.isnull().sum())  # Check for NaN values\n",
    "print(\"\\nEmpty string values in test_df:\")\n",
    "for col in test_df.select_dtypes(include=['object']):  # Iterate over columns with string datatype\n",
    "    print(f\"{col}: {(test_df[col] == '').sum()}\")     # Count empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with nulls or empty strings in Clade, ignore Lineage nulls/empty strings\n",
    "test_df.replace('', pd.NA, inplace=True)  # Replace empty strings with NaN\n",
    "test_df.dropna(subset=['Clade'], inplace=True)  # Drop rows where Clade is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and empty strings\n",
    "print(\"NaN values in test_df:\")\n",
    "print(test_df.isnull().sum())  # Check for NaN values\n",
    "print(\"\\nEmpty string values in test_df:\")\n",
    "for col in test_df.select_dtypes(include=['object']):  # Iterate over columns with string datatype\n",
    "    print(f\"{col}: {(test_df[col] == '').sum()}\")     # Count empty strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.5 Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clade Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Clade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with \"unassigned\" in 'Clade' from the training data\n",
    "test_df = test_df[test_df['Clade'] != 'unassigned']  # Filter out rows with label \"unassigned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HACK: Drop rows in 'Clade' from the test data, as it is not present in the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary HACK: Drop rows with \"2.3.2.1c\" in 'Clade' from the test data (TODO: which test data?), \n",
    "# as it is not present in the training data \n",
    "test_df = test_df[test_df['Clade'] != '2.3.2.1c']  # Filter out rows with label \"2.3.2.1c\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary HACK: Drop rows with \"Am_nonGsGD\" in 'Clade' from the test data (TODO: which test data?), \n",
    "# as it is not present in the training data \n",
    "test_df = test_df[test_df['Clade'] != 'Am_nonGsGD']  # Filter out rows with label \"Am_nonGsGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary HACK: Drop rows with \"3C.2a1b.2a.1a.1\" in 'Clade' from the test data (EpiFlu_Test/11-Aug-2024_Oceania/gisaid_epiflu_sequence_11-Aug-2024.fasta), \n",
    "# as it is not present in the training data (EpiFlu_Training/01-Aug-2024/gisaid_epiflu_sequence_01-Aug-2024_All_Hosts_Type_A_USA.fasta) \n",
    "test_df = test_df[test_df['Clade'] != '3C.2a1b.2a.1a.1']  # Filter out rows with label \"3C.2a1b.2a.1a.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary HACK: Drop rows with \"2.3.2.1a\" in 'Clade' from the test data (EpiFlu_Test/11-Aug-2024_Oceania/gisaid_epiflu_sequence_11-Aug-2024.fasta), \n",
    "# as it is not present in the training data (EpiFlu_Training/01-Aug-2024/gisaid_epiflu_sequence_01-Aug-2024_All_Hosts_Type_A_USA.fasta) \n",
    "test_df = test_df[test_df['Clade'] != '2.3.2.1a']  # Filter out rows with label \"2.3.2.1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Clade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H_Subtype Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['H_Subtype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N_Subtype Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['N_Subtype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.6 Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.7 Look at sequence length stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ha_sequence_lengths = test_df.apply(get_sequence_length, axis=1, column=\"HA\")\n",
    "new_na_sequence_lengths = test_df.apply(get_sequence_length, axis=1, column=\"NA\")\n",
    "print(\"Sequence lengths in HA columns:\")\n",
    "new_ha_sequence_lengths.describe()\n",
    "print(\"Sequence lengths in NA columns:\")\n",
    "new_na_sequence_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 K-mers and k-mer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store kmers for each sequence in the new dataset (identified by row index)\n",
    "test_kmer_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract k-mers from each sequence and store them in the dictionary\n",
    "for i, row in test_df.iterrows():\n",
    "    # Extract kmers from HA sequence (assuming it exists)\n",
    "    ha_kmers = []\n",
    "    if \"HA\" in row:  # Check if \"HA\" column exists\n",
    "        sequence = str(row[\"HA\"])\n",
    "        ha_kmers = get_kmers(sequence, kmer_length)\n",
    "\n",
    "    # Store kmers separately in the dictionary\n",
    "    test_kmer_dict[i] = {\n",
    "        \"HA\": ha_kmers\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 One-Hot Encoding using chunking to optimize memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding on new data using the same kmer chunking logic as before to optimize memory usage\n",
    "# Apply the same k-mer extraction and chunking as with training data\n",
    "test_ha_features = []\n",
    "for i in range(0, len(test_kmer_dict), chunk_size):\n",
    "    # Get a chunk of data\n",
    "    chunk_dict = dict(list(test_kmer_dict.items())[i:i + chunk_size])\n",
    "    \n",
    "    # Process features for the chunk\n",
    "    chunk_ha_features = process_chunk(chunk_dict)\n",
    "    \n",
    "    # Append features from the chunk\n",
    "    test_ha_features.extend(chunk_ha_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "# Convert list of lists to numpy array\n",
    "X_new = np.array(test_ha_features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "y_pred_new = model.predict(X_new)\n",
    "\n",
    "# Decode predictions to get the original Clade labels\n",
    "predicted_clades = le.inverse_transform(y_pred_new)\n",
    "\n",
    "# Add predicted clades back to new_df\n",
    "test_df['Predicted_Clade'] = predicted_clades\n",
    "\n",
    "# Display the data with predictions and true clades\n",
    "display(test_df[['Isolate_Name', 'Isolate_ID', 'Collection Date', 'Clade', 'Predicted_Clade']])  # Display true and predicted clades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and display metrics \n",
    "y_true_new = le.transform(test_df['Clade']) # Encode true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Predictions\n",
    "print(\"\\n### Model Evaluation on Test Dataset ###\")\n",
    "\n",
    "# Accuracy: Proportion of correctly predicted samples\n",
    "accuracy = accuracy_score(y_true_new, y_pred_new)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision: Ratio of true positives to all predicted positives\n",
    "precision = precision_score(y_true_new, y_pred_new, average='weighted') # Weighted average for multi-class\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Recall: Ratio of true positives to all actual positives\n",
    "recall = recall_score(y_true_new, y_pred_new, average='weighted') # Weighted average for multi-class\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# F1-score: Harmonic mean of precision and recall\n",
    "f1 = f1_score(y_true_new, y_pred_new, average='weighted') # Weighted average for multi-class\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_new, y_pred_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix with Seaborn\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true_new, y_pred_new)\n",
    "\n",
    "# Create a new figure for the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create heatmap using seaborn\n",
    "heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\") # Customize heatmap with annotations, format, and colormap\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predicted Clade\")\n",
    "plt.ylabel(\"True Clade\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show the confusion matrix\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End the timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End timing\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print total testing runtime\n",
    "testing_time = end_time - start_time\n",
    "print(f\"\\nTotal testing runtime: {format_runtime(testing_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the predicted clades for each subtype? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Subtype' by concatenating 'H_Subtype' and 'N_Subtype'\n",
    "test_df['Subtype'] = test_df['H_Subtype'] + test_df['N_Subtype']\n",
    "\n",
    "# Group by 'Subtype' and collect unique 'Predicted_Clade' values into a list\n",
    "grouped_data = test_df.groupby('Subtype')['Predicted_Clade'].unique().reset_index()\n",
    "\n",
    "# Rename the 'Predicted_Clade' column to 'Predicted Clades'\n",
    "grouped_data = grouped_data.rename(columns={'Predicted_Clade': 'Predicted Clades'})\n",
    "\n",
    "# Convert NumPy arrays in 'Predicted Clades' to strings\n",
    "grouped_data['Predicted Clades'] = grouped_data['Predicted Clades'].astype(str)\n",
    "grouped_data\n",
    "\n",
    "# Display the grouped_data directly in markdown format\n",
    "print(grouped_data.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new column 'Subtype' by concatenating 'H_Subtype' and 'N_Subtype'\n",
    "# test_df['Subtype'] = test_df['H_Subtype'] + test_df['N_Subtype']\n",
    "\n",
    "# Group by 'Subtype' and collect 'Predicted_Clade' values\n",
    "grouped_data = test_df.groupby(['Subtype', 'Predicted_Clade']).size().reset_index(name='Count')\n",
    "\n",
    "# Calculate total count for the entire test dataset\n",
    "total_count = len(test_df) \n",
    "\n",
    "# Calculate percentage using the total count of the test dataset\n",
    "grouped_data['Percentage'] = ((grouped_data['Count'] / total_count) * 100).round(1)\n",
    "\n",
    "# Rename the 'Predicted_Clade' column to 'Predicted Clades'\n",
    "grouped_data = grouped_data.rename(columns={'Predicted_Clade': 'Predicted Clades'})\n",
    "grouped_data\n",
    "\n",
    "# Display the grouped_data directly in markdown format\n",
    "print(grouped_data.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new column 'Subtype' by concatenating 'H_Subtype' and 'N_Subtype'\n",
    "# test_df['Subtype'] = test_df['H_Subtype'] + test_df['N_Subtype']\n",
    "\n",
    "# Group by 'Subtype' and collect 'Predicted_Clade' values\n",
    "grouped_data = test_df.groupby(['Subtype', 'Predicted_Clade']).size().reset_index(name='Count')\n",
    "\n",
    "# Calculate total count for the entire test dataset\n",
    "total_count = len(test_df) \n",
    "\n",
    "# Calculate percentage using the total count of the test dataset\n",
    "grouped_data['Percentage'] = ((grouped_data['Count'] / total_count) * 100).round(1)\n",
    "\n",
    "# Rename the 'Predicted_Clade' column to 'Predicted Clades'\n",
    "grouped_data = grouped_data.rename(columns={'Predicted_Clade': 'Predicted Clades'})\n",
    "\n",
    "# Aggregate Predicted Clades, Count and Percentage for each Subtype\n",
    "grouped_data = grouped_data.groupby('Subtype').agg({\n",
    "    'Predicted Clades': lambda x: ', '.join(x), \n",
    "    'Count': lambda x: ', '.join(x.astype(str)), \n",
    "    'Percentage': lambda x: ', '.join(x.astype(str))\n",
    "}).reset_index()\n",
    "\n",
    "# Display the grouped_data directly in markdown format\n",
    "print(grouped_data.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-flu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
